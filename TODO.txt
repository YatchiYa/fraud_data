Liens utiles : 

https://github.com/fariha123/IEEE-CIS-Fraud-Detection/blob/main/EDA%20for%20IEEE%20CIS%20Fraud%20Detection.ipynb
https://nbviewer.org/github/Abrar2652/IEEE-CIS-Fraud-Detection-Project/blob/main/ieee-cis-fraud-detection.ipynb
https://medium.com/@mr.priyankmishra/a-realistic-approach-to-ieee-cis-fraud-detection-25faea54137
https://codelabs.developers.google.com/codelabs/frauddetection-using-console#0
https://towardsdatascience.com/google-cloud-storage-gcs-to-bigquery-the-simple-way-4bb74216b8c8
https://gcloud.devoteam.com/blog/how-i-used-google-cloud-data-fusion-to-create-a-data-warehouse-part-2/
https://bharathkp.medium.com/loading-data-into-bigquery-bq-from-google-cloud-storage-gcs-using-cloud-functions-3e03c189a65d


https://lindevs.com/download-dataset-from-kaggle-using-api-and-python/
https://www.ternarydata.com/news/use-python-and-google-cloud-to-schedule-a-file-download-and-load-into-bigquery-3p3aw

---------------------------------------------------------------------------------------

TODO : 

-> strategie utilisé : remplacé les données de la table directement.
-> TODO : trouver un moyen de telecharger des données de kaggle avec python directement.
-> creer une cloud function qui telecharge les données directement de kaggle vers GCS
-> creer un pub/sub qui va declancher l'export du GCS vers BQ
-> TODO : A DEVELOPPER : gerer le versionnage des fichiers.


--------------------------------------

****architecture 1 : Cloud Functions + BigQuery = Data Feed Automation

1 - create a project : fraud-detection-simulation
2 - create a bucket on cloud storage : fraud-data-lake
3 - create a cloud function that transfer from google storage to bq

function to use : 

from google.cloud import bigquery

def main(event, context):
  project_id = "fraud-detection-simulation"
  bq_dataset = "data"
  # TODO : get table name
  tmp_name =  event["name"]
  tmp_array = tmp_name.split('.')[:-1]
  bq_table = ''.join(tmp_array)
  # TODO : file name & bucket
  file_name = event["name"]
  bucket_name = event["bucket"]
  gcs_uri = "gs://{}/{}".format(bucket_name, file_name)
  table_id = "{}.{}.{}".format(project_id, bq_dataset, bq_table)

  client = bigquery.Client()

  job_config = bigquery.LoadJobConfig(
      autodetect=True,
      write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
      skip_leading_rows=1, source_format=bigquery.SourceFormat.CSV)

  load_job = client.load_table_from_uri(
      gcs_uri, table_id, job_config=job_config
  )

  load_job.result()





4 - depot du fichie en cloud storag GCS
5 - verifier sur BigQuery que la table est crée.





