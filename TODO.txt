Liens utiles : 

https://github.com/fariha123/IEEE-CIS-Fraud-Detection/blob/main/EDA%20for%20IEEE%20CIS%20Fraud%20Detection.ipynb
https://nbviewer.org/github/Abrar2652/IEEE-CIS-Fraud-Detection-Project/blob/main/ieee-cis-fraud-detection.ipynb
https://medium.com/@mr.priyankmishra/a-realistic-approach-to-ieee-cis-fraud-detection-25faea54137
https://codelabs.developers.google.com/codelabs/frauddetection-using-console#0
https://towardsdatascience.com/google-cloud-storage-gcs-to-bigquery-the-simple-way-4bb74216b8c8
https://gcloud.devoteam.com/blog/how-i-used-google-cloud-data-fusion-to-create-a-data-warehouse-part-2/
https://bharathkp.medium.com/loading-data-into-bigquery-bq-from-google-cloud-storage-gcs-using-cloud-functions-3e03c189a65d
https://medium.com/google-cloud/bigquery-explained-data-ingestion-cdc26a588d0
https://lindevs.com/download-dataset-from-kaggle-using-api-and-python/
https://www.ternarydata.com/news/use-python-and-google-cloud-to-schedule-a-file-download-and-load-into-bigquery-3p3aw

---------------------------------------------------------------------------------------

TODO : 

-> strategie utilisé : remplacé les données de la table directement.
-> TODO : trouver un moyen de telecharger des données de kaggle avec python directement.
-> TODO : A DEVELOPPER : gerer le versionnage des fichiers, ingestion du fichier en cloud storage
-> TODO : A DEVELOPPER : gérer le versionnage en bigquery



--------------------------------------

****architecture 1 : Depot fichier(datalake) + Cloud scheduler + pub/sub (cloud function github -> GCS)  + Cloud Functions GCS + BigQuery = Data Feed Automation
1 - Creer une cloud function (pub/sub) git vers storage

function to use : 

import os
import wget

from google.cloud import storage

url = os.environ['URL']
bucket_name = os.environ['BUCKET'] #without gs://
file_name = os.environ['FILE_NAME']

cf_path = '/tmp/{}'.format(file_name)

def import_file(event, context):
     # set storage client
     client = storage.Client()
     
     # get bucket
     bucket = client.get_bucket(bucket_name)
     
     # download the file to Cloud Function's tmp directory
     wget.download(url, cf_path)
     
     # set Blob
     blob = storage.Blob(file_name, bucket)
     
     # upload the file to GCS
     blob.upload_from_filename(cf_path)
     
     print("""This Function was triggered by messageId {} published at {}
     """.format(context.event_id, context.timestamp))


2 - creer un cloud scheduler pour lancer la cloud function
3 - create a project : fraud-detection-simulation
4 - create a bucket on cloud storage : fraud-data-lake
5 - create a cloud function that transfer from google storage to bq

function to use : 

from google.cloud import bigquery

def main(event, context):
  project_id = "fraud-detection-simulation"
  bq_dataset = "data"
  # TODO : get table name
  tmp_name =  event["name"]
  tmp_array = tmp_name.split('.')[:-1]
  bq_table = ''.join(tmp_array)
  # TODO : file name & bucket
  file_name = event["name"]
  bucket_name = event["bucket"]
  gcs_uri = "gs://{}/{}".format(bucket_name, file_name)
  table_id = "{}.{}.{}".format(project_id, bq_dataset, bq_table)

  client = bigquery.Client()

  job_config = bigquery.LoadJobConfig(
      autodetect=True,
      write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
      skip_leading_rows=1, source_format=bigquery.SourceFormat.CSV)

  load_job = client.load_table_from_uri(
      gcs_uri, table_id, job_config=job_config
  )

  load_job.result()





6 - depot du fichie en cloud storag GCS
7 - verifier sur BigQuery que la table est crée.




